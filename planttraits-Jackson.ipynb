{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 1  # Verbosity\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
    "    image_size = [224, 224]  # Input image size\n",
    "    epochs = 12 # Training epochs\n",
    "    batch_size = 96  # Batch size\n",
    "    lr_mode = \"step\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    num_classes = 6 # Number of classes in the dataset\n",
    "    num_folds = 5 # Number of folds to split the dataset\n",
    "    fold = 0 # Which fold to set as validation data\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean',\n",
    "                   'X26_mean', 'X50_mean', 'X3112_mean',]\n",
    "    aux_class_names = list(map(lambda x: x.replace(\"mean\",\"sd\"), class_names))\n",
    "    num_classes = len(class_names)\n",
    "    aux_num_classes = len(aux_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/blue/esi4611/share/planttraits2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + Valid\n",
    "train = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "#train['image_path'] = f'{BASE_PATH}/train_images/'+train['id'].astype(str)+'.jpeg'\n",
    "train.loc[:, CFG.aux_class_names] = train.loc[:, CFG.aux_class_names].fillna(-1)\n",
    "display(train.head(2))\n",
    "\n",
    "# Test\n",
    "test = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "#test['image_path'] = f'{BASE_PATH}/test_images/'+test['id'].astype(str)+'.jpeg'\n",
    "FEATURE_COLS = test.columns[1:-1].tolist()\n",
    "display(test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "image_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Define the function to create a TensorFlow dataset for images\n",
    "def create_dataset(image_paths, batch_size=128):\n",
    "    def process_path(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    image_ds = path_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    image_ds = image_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return image_ds\n",
    "\n",
    "def extract_features_with_dataset(dataset, df):\n",
    "    features_list = []\n",
    "    for batch_imgs in dataset:\n",
    "        print(\".\", end=\"\")  # Print progress\n",
    "        features = image_model.predict(batch_imgs, verbose=0)\n",
    "        features_list.extend(features)\n",
    "    features_array = np.array(features_list)\n",
    "    \n",
    "    # Convert the features array into a DataFrame\n",
    "    features_df = pd.DataFrame(features_array)\n",
    "    \n",
    "    features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n",
    "    \n",
    "    new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_folder = '/blue/esi4611/share/planttraits2024/train_images'\n",
    "\n",
    "image_paths = [os.path.join(train_image_folder, f\"{img_id}.jpeg\") for img_id in train['id']]\n",
    "\n",
    "# Create the dataset\n",
    "image_dataset = create_dataset(image_paths)\n",
    "\n",
    "# Extract features and directly insert them into the DataFrame as separate columns\n",
    "train = extract_features_with_dataset(image_dataset, train)\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do cross-validation testing (this is relatively slow)\n",
    "do_cv = True\n",
    "\n",
    "X_full = train.drop(columns=mean_columns)\n",
    "Y_full = train[mean_columns]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for column in Y_full.columns:\n",
    "\n",
    "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    if do_cv:\n",
    "        print(f\"\\nDoing cross-validation scoring for {column}...\")\n",
    "        scores = cross_val_score(model, X_full, Y_full[column],\n",
    "                                 cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "                                 scoring='r2')        \n",
    "        print(f\"R^2 score for {column}: {np.mean(scores)}\")\n",
    "    \n",
    "    #train model with all data\n",
    "    print(f\"Training model for {column}...\")\n",
    "    model.fit(X_full, Y_full[column])\n",
    "    models[column] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_paths = test_df.image_path.values\n",
    "test_features = scaler.transform(test_df[FEATURE_COLS].values) \n",
    "test_ds = build_dataset(test_paths, test_features, batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = test_df[[\"id\"]].copy()\n",
    "target_cols = [x.replace(\"_mean\",\"\") for x in CFG.class_names]\n",
    "pred_df[target_cols] = preds.tolist()\n",
    "\n",
    "sub_df = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\n",
    "sub_df = sub_df[[\"id\"]].copy()\n",
    "sub_df = sub_df.merge(pred_df, on=\"id\", how=\"left\")\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
